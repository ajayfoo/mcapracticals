{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYnCuZi9yG7vbZdgbu0O1D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Practical No. 9\n","**Theory:**  \n","Machine Learning (ML) deployment refers to the process of making trained ML models available for use in production environments. It involves taking the models developed during the ML development phase and deploying them to serve predictions or perform inference on new, unseen data.\n","\n","Here are some key aspects and considerations in ML deployment:\n","\n","1. Model Packaging: ML models need to be packaged in a format that can be easily deployed and consumed by the target environment. Common formats include serialized files (e.g., pickle, ONNX), containerized images (e.g., Docker), or specific formats supported by ML deployment platforms.\n","\n","2. Infrastructure and Environment: ML models require appropriate infrastructure and computing resources to run efficiently. This may involve provisioning servers, cloud instances, or using specialized ML deployment platforms that handle scaling, load balancing, and resource management.\n","\n","3. Deployment Options: ML models can be deployed in various ways, depending on the requirements and constraints of the application. Options include cloud-based deployments (e.g., AWS, Azure, Google Cloud), on-premises deployments, edge deployments (running models on edge devices), or using specialized ML deployment platforms.\n","\n","4. Model Serving: ML models need to be served so that they can receive input data and provide predictions or inference results. This typically involves exposing APIs or endpoints that can be accessed by client applications or systems. Common protocols used for serving models include RESTful APIs, gRPC, or custom interfaces.\n","\n","5. Scalability and Performance: ML deployment should consider scalability to handle varying workloads and ensure good performance. This may involve techniques such as load balancing, distributed computing, and optimizing the inference process for efficient resource utilization.\n","\n","6. Monitoring and Maintenance: Once deployed, ML models need to be monitored to ensure they are functioning correctly, performant, and producing accurate results. Monitoring may involve tracking metrics like prediction latency, throughput, error rates, and data drift. Regular maintenance and updates may also be required as new data becomes available or improvements to the models are made.\n","\n","7. Security and Privacy: ML deployment must consider security and privacy aspects to protect sensitive data and prevent unauthorized access or attacks. This may involve securing the model serving endpoints, encrypting data in transit and at rest, and following best practices for handling sensitive information.\n","\n","ML deployment is a critical step in the ML lifecycle, as it allows organizations to realize the value of their ML models by integrating them into real-world applications and systems. Effective deployment ensures reliable and efficient usage of ML models and enables data-driven decision-making in various domains."],"metadata":{"id":"OMaeXdl61lkM"}},{"cell_type":"markdown","source":["## Machine Learning model for a web service\n","**Theory:**  \n","A machine learning web service is a service that provides machine learning capabilities over the internet. It allows users to access and utilize machine learning models and algorithms through an application programming interface (API).\n","\n","Here are some key components and characteristics of a machine learning web service:\n","\n","1. Model Deployment: The web service hosts trained machine learning models and makes them available for consumption by client applications.\n","\n","2. API Interface: It provides a well-defined API that allows clients to interact with the machine learning models. The API typically supports standard HTTP methods such as GET, POST, PUT, and DELETE for performing various operations.\n","\n","3. Input and Output Formats: The web service defines the expected input format for making predictions or running inference on the models. It also specifies the format of the output response, which could be in JSON, XML, or other commonly used formats.\n","\n","4. Scalability: Machine learning web services are designed to handle a large number of requests concurrently and provide scalability to accommodate varying loads and user demands.\n","\n","5. Security: It incorporates security measures to protect the models and the data they process. This may include authentication and authorization mechanisms, encryption, and secure communication protocols.\n","\n","6. Versioning and Model Updates: Web services often support versioning to manage changes and updates to the deployed models. This allows for backward compatibility and smooth transitions when introducing new versions of models.\n","\n","7. Monitoring and Analytics: The web service may provide monitoring and analytics capabilities to track usage statistics, performance metrics, and other relevant information for model evaluation and improvement.\n","\n","Machine learning web services are widely used in various applications, including predictive analytics, recommendation systems, fraud detection, natural language processing, and image recognition. They enable developers and data scientists to leverage machine learning capabilities without having to worry about the infrastructure and deployment complexities, making it easier to integrate machine learning into their applications."],"metadata":{"id":"mDQBIfyw14qT"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGFcTmKW1M_T","executionInfo":{"status":"ok","timestamp":1686511393955,"user_tz":-330,"elapsed":403,"user":{"displayName":"Ajay Thevar","userId":"12926889717356051842"}},"outputId":"2c9dec68-c438-48f2-a43d-911c761b4e57"},"outputs":[{"output_type":"stream","name":"stdout","text":["['setosa' 'versicolor' 'virginica']\n","['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","   sepallength  sepalwidth  petallength  petalwidth  species\n","0          5.1         3.5          1.4         0.2        0\n","1          4.9         3.0          1.4         0.2        0\n","2          4.7         3.2          1.3         0.2        0\n","3          4.6         3.1          1.5         0.2        0\n","4          5.0         3.6          1.4         0.2        0\n","\n","ACCURACY OF THE MODEL:  0.9555555555555556\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0])"]},"metadata":{},"execution_count":3}],"source":["# importing required libraries\n","# importing Scikit-learn library and datasets package\n","from sklearn import datasets\n","\n","# Loading the iris plants dataset (classification)\n","iris = datasets.load_iris()\t\n","print(iris.target_names)\n","print(iris.feature_names)\n","# dividing the datasets into two parts i.e. training datasets and test datasets\n","X, y = datasets.load_iris( return_X_y = True)\n","\n","# Splitting arrays or matrices into random train and test subsets\n","from sklearn.model_selection import train_test_split\n","# i.e. 70 % training dataset and 30 % test datasets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n","# importing random forest classifier from assemble module\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","# creating dataframe of IRIS dataset\n","data = pd.DataFrame({'sepallength': iris.data[:, 0], 'sepalwidth': iris.data[:, 1],\n","\t\t\t\t\t'petallength': iris.data[:, 2], 'petalwidth': iris.data[:, 3],\n","\t\t\t\t\t'species': iris.target})\n","# printing the top 5 datasets in iris dataset\n","print(data.head())\n","# creating a RF classifier\n","clf = RandomForestClassifier(n_estimators = 100)\n","\n","# Training the model on the training dataset\n","# fit function is used to train the model using the training sets as parameters\n","clf.fit(X_train, y_train)\n","\n","# performing predictions on the test dataset\n","y_pred = clf.predict(X_test)\n","\n","# metrics are used to find accuracy or error\n","from sklearn import metrics\n","print()\n","\n","# using metrics module for accuracy calculation\n","print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n"]},{"cell_type":"markdown","source":["### Save The Model"],"metadata":{"id":"i4-PTkwF6n1K"}},{"cell_type":"code","source":["import joblib\n","joblib.dump(clf, 'classifier.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkNGEC986tv1","executionInfo":{"status":"ok","timestamp":1686511523197,"user_tz":-330,"elapsed":400,"user":{"displayName":"Ajay Thevar","userId":"12926889717356051842"}},"outputId":"98ddad1d-b131-4190-9bba-3c31eda73071"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['classifier.pkl']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## Deploying machine learning models on edge devices as embedded models: TFLite\n","\n","**Theory:**  \n","Deploying machine learning models on edge devices as embedded models refers to the process of running trained ML models directly on edge devices such as smartphones, IoT devices, embedded systems, or other resource-constrained hardware. Instead of relying on cloud or remote servers for inference, the models are integrated into the edge devices themselves, allowing for real-time, offline, or low-latency inference.\n","\n","Here are some key considerations and benefits of deploying ML models as embedded models on edge devices:\n","\n","1. Reduced Latency: By running ML models directly on edge devices, latency is minimized as there is no need to send data to remote servers for inference. This enables real-time or near-real-time processing, which is crucial for applications where low latency is essential, such as real-time object detection or voice recognition.\n","\n","2. Privacy and Security: Deploying ML models on edge devices eliminates the need to send sensitive data to external servers for processing. This helps maintain data privacy and reduces potential security risks associated with transmitting data over networks. The data stays on the device, enhancing privacy and complying with data protection regulations.\n","\n","3. Offline Capability: Embedded ML models can operate offline without requiring a constant internet connection. This is beneficial in scenarios where internet connectivity is limited, unstable, or expensive. Applications like voice assistants, mobile apps, or industrial IoT devices can continue to function even in offline environments.\n","\n","4. Bandwidth Optimization: By performing inference on the edge device itself, the amount of data transmitted over the network is significantly reduced. Only relevant or processed results are sent, optimizing bandwidth usage and reducing the load on network infrastructure.\n","\n","5. Real-time Decision Making: With embedded ML models, edge devices can make autonomous, real-time decisions without relying on cloud or remote servers. This is advantageous in applications where quick decision-making is critical, such as autonomous vehicles, industrial automation, or healthcare devices.\n","\n","6. Enhanced Reliability: Deploying models on edge devices increases reliability by reducing dependence on external servers. It mitigates issues related to network connectivity, server downtime, or latency fluctuations, ensuring continuous and consistent operation of the ML models.\n","\n","7. Resource Constraints: Edge devices often have limited computational resources, memory, or power constraints. Optimizing ML models for deployment on such devices requires techniques like model compression, quantization, or pruning to reduce the model size and computational requirements while maintaining acceptable accuracy levels.\n","\n","Deploying ML models as embedded models on edge devices brings numerous benefits, including reduced latency, improved privacy and security, offline capability, bandwidth optimization, and real-time decision-making. It enables a wide range of applications that require local processing, responsiveness, and autonomy, opening up opportunities for edge computing in various domains."],"metadata":{"id":"Wx6L4vuJ7wSY"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create a model using high-level tf.keras.* APIs\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(units=1, input_shape=[1]),\n","    tf.keras.layers.Dense(units=16, activation='relu'),\n","    tf.keras.layers.Dense(units=1)\n","])\n","model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model\n","model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5) # train the model\n","# (to generate a SavedModel) tf.saved_model.save(model, \"saved_model_keras_dir\")\n","\n","# Convert the model.\n","converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","# Save the model.\n","with open('model.tflite', 'wb') as f:\n","  f.write(tflite_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQVLMRT57zAL","executionInfo":{"status":"ok","timestamp":1686511842163,"user_tz":-330,"elapsed":13402,"user":{"displayName":"Ajay Thevar","userId":"12926889717356051842"}},"outputId":"9e94cf56-6871-4b66-ea86-0a1abf347c11"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1/1 [==============================] - 1s 1s/step - loss: 4.5039\n","Epoch 2/5\n","1/1 [==============================] - 0s 17ms/step - loss: 4.0360\n","Epoch 3/5\n","1/1 [==============================] - 0s 14ms/step - loss: 3.6359\n","Epoch 4/5\n","1/1 [==============================] - 0s 16ms/step - loss: 3.2773\n","Epoch 5/5\n","1/1 [==============================] - 0s 13ms/step - loss: 2.9522\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"]}]}]}